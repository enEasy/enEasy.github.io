[{"title":"Hbase语法实验","url":"/2021/05/08/Hbase语法实验/","content":"\n\n\n\n\n# Hbase实验二\n\n## 1、一般操作\n\n### 查询服务器状态\n\n```\nstatus\n```\n\n![1620197294908](../images/Hbase语法实验/1620197294908.png)\n\n### 查询Hbase版本\n\n```\nversion\n```\n\n### 查看所有表\n\n```\nversion\n```\n\n![1620199317627](../images/Hbase语法实验/1620199317627.png)\n\n<!--more-->\n\n## 2、增删改\n\n### 创建一个表\n\n```\ncreate 'member057','member_id','address','info'\n```\n\n![1620199328321](../images/Hbase语法实验/1620199328321.png)\n\n![img](file://D:/Blog/source/_posts/Hbase%E5%AE%9E%E9%AA%8C%E4%BA%8C.assets/1620199317627.png?lastModify=1620442179)\n\n### 获得表的描述\n\n```\ndescribe 'member057' \n```\n\n![1620199337845](Hbase实验二.assets/1620199337845.png)\n\n### 添加一个列族\n\n```\nalter 'member057','id'\n```\n\n![1620199345105](Hbase实验二.assets/1620199345105.png)\n\n### 添加数据\n\n- ### 在HBase  shell中，我们可以通过put命令来插入数据。列族下的列不需要提前创建，在需要时通过：来指定即可。添加数据如下\n\n  ```\n  put 'member057', 'debugo','id','11'\n  put 'member057', 'debugo','info:age','27'\n  put 'member057', 'debugo','info:birthday','1991-04-04'\n  put 'member057', 'debugo','info:industry', 'it'\n  put 'member057', 'debugo','address:city','Shanghai'\n  put 'member057', 'debugo','address:country','China'\n  put 'member057', 'Sariel', 'id', '21'\n  put 'member057', 'Sariel','info:age', '26'\n  put 'member057', 'Sariel','info:birthday', '1992-05-09'\n  put 'member057', 'Sariel','info:industry', 'it'\n  put 'member057', 'Sariel','address:city', 'Beijing'\n  put 'member057', 'Sariel','address:country', 'China'\n  put 'member057', 'Elvis', 'id', '22'\n  put 'member057', 'Elvis','info:age', '26'\n  put 'member057', 'Elvis','info:birthday', '1992-09-14'\n  put 'member057', 'Elvis','info:industry', 'it'\n  put 'member057', 'Elvis','address:city', 'Beijing'\n  put 'member057', 'Elvis','address:country', 'china\n  ```\n\n  ![1620199354966](Hbase实验二.assets/1620199354966.png)\n\n### 查看表数据\n\n```\nscan 'member057'\n```\n\n![1620199360333](Hbase实验二.assets/1620199360333.png)\n\n### 删除一个列族\n\n```\nalter 'member057', {NAME => 'member_id', METHOD => 'delete'\n```\n\n![1620199365309](Hbase实验二.assets/1620199365309.png)\n\n### 删除列\n\n- 通过delete命令，我们可以删除id为某个值'info:age' 字段，接下来的get就无值了：\n\n  ```\n  delete 'member057','debugo','info:age'\n  get 'member057','debugo','info:age'\n  ```\n\n  ![1620199370489](Hbase实验二.assets/1620199370489.png)\n\n- 删除整行的值，用deleteall命令\n\n  ```\n  deleteall 'member057','debugo'\n  get 'member057','debugo'\n  ```\n\n  ![1620199374998](Hbase实验二.assets/1620199374998.png)\n\n- 通过enable和disable来启用/禁用这个表，相应的可以通过is_enabled和is_disabled来检查表是否被禁用\n\n  ```\n  is_enabled 'member057'\n  is_disabled 'member057'\n  ```\n\n  ![1620199379588](Hbase实验二.assets/1620199379588.png)\n\n  ![1620199450690](Hbase实验二.assets/1620199450690.png)\n\n- 使用exists来检查表是否存在\n\n  ```\n  exists 'member057'\n  ```\n\n  ![1620199457606](Hbase实验二.assets/1620199457606.png)\n\n- 删除表需要先将表disable\n\n  ```\n  disable 'member057'\n  drop 'member057'\n  ```\n\n  ![1620199462001](Hbase实验二.assets/1620199462001.png)\n\n## 3、查询\n\n### 查询表中有多少行，用count命令：\n\n```\ncount 'member057'\n```\n\n![1620199466891](Hbase实验二.assets/1620199466891.png)\n\n### get\n\n- 获取一个id的所有数据\n\n  ```\n  get 'member057', 'Sariel'\n  ```\n\n  ![1620199472428](Hbase实验二.assets/1620199472428.png)\n\n- 获得一个id，一个列族（一个列）中的所有数据：\n\n  ```\n  get 'member057', 'Sariel', 'info'\n  ```\n\n  ![1620199476998](Hbase实验二.assets/1620199476998.png)\n\n### 查询整表数据\n\n```\nscan 'member057'\n```\n\n![1620199481320](Hbase实验二.assets/1620199481320.png)\n\n### 扫描整个列族\n\n```\nscan 'member057', {COLUMN=>'info'}\n```\n\n![1620199492029](Hbase实验二.assets/1620199492029.png)\n\n### 指定扫描期中的某个列\n\n```\nscan 'member057', {COLUMNS=> 'info:birthday'}\n```\n\n![1620199498972](Hbase实验二.assets/1620199498972.png)\n\n- ### 除了列（COLUMNS）修饰词外，HBase 还支持 Limit（限制查询结果行数），STARTROW（ROWKEY 起始行。会先根据这个 key 定位到 region，再向后扫描）、STOPROW(结束行)、TIMERANGE（限定时间戳范围）、VERSIONS（版本数）、和 FILTER（按条件过滤行）等。比如我们从 Sariel 这个 rowkey 开始，找下一个行的最新版本：\n\n  ```\n  scan 'member057', { STARTROW => 'Sariel', LIMIT=>1,\n  VERSIONS=>1}\n  ```\n\n  ![1620199504249](Hbase实验二.assets/1620199504249.png)\n\n### Filter 是一个非常强大的修饰词，可以设定一系列条件来进行过滤。比如我们要限制某个列的值等于 26\n\n```\nscan 'member057', FILTER=>\"ValueFilter(=,'binary:26')\"\n```\n\n![1620199511422](Hbase实验二.assets/1620199511422.png)\n\n### 值包含 6 这个值：\n\n```\nscan 'member057', FILTER=>\"ValueFilter(=,'substring:6')\"\n```\n\n![1620199519312](Hbase实验二.assets/1620199519312.png)\n\n### 列名中的前缀为 birth 的\n\n```\nscan 'member057', FILTER=>\"ColumnPrefixFilter('birth') \"\n```\n\n![1620199523851](Hbase实验二.assets/1620199523851.png)\n\n### FILTER 中支持多个过滤条件通过括号、AND 和 OR 的条件组合\n\n```\nscan 'member057', FILTER=>\"ColumnPrefixFilter('birth') AND\nValueFilter ValueFilter(=,'substring:1988')\"\n```\n\n\n\n### PrefixFilter 是对 Rowkey 的前缀进行判断,这是一个非常常用的功能。\n\n```\nscan 'member057', FILTER=>\"PrefixFilter('E')\"\n```\n\n![1620199530370](Hbase实验二.assets/1620199530370.png)","tags":["-hadoop"],"categories":["hadoop"]},{"title":"hadoop伪分布式安装","url":"/2021/05/08/hadoop伪分布式安装/","content":"\n# hadoop伪分布式安装\n\n## 1、安装jdk\n\n### 安装JDK\n\n可以将所需的软件安装包放在一块，例如：~/software\n\n#### （1）准备软件\n\nJDK的安装包已经为大家准备好，在/root/software目录下，可以使用如下命令进行查看：\n\n```\ncd /root/software/\t\t# 进入目录\nll\t\t\t\t\t# 罗列出当前文件或目录的详细信息，是ls -l的别名\n```\n\n#### （2）解压压缩包\n\n```\ntar -zxvf jdk-8u221-linux-x64.tar.gz\n```\n\n#### （3）在此处我们配置**系统环境变量**，使用命令：\n\n```\nvim /etc/profile或者\nvim ~/.bashrc                  //建议\n\n```\n\n<!--more-->\n\n#### （4）在最后加入以下两行内容：\n\n**路径为自己jdk路径**\n\n```\nexport JAVA_HOME=/root/software/jdk1.8.0_221  # 配置Java的安装目录，路径为自己jdk路径\nexport PATH=$PATH:$JAVA_HOME/bin  # 在原PATH的基础上加入JDK的bin目录\n```\n\n一定要注意PATH值的修改，**一定要在引用原PATH值**，否则Linux的很多操作命令就不能使用了。\n\n**注意：**\n\n- export 是把这两个变量导出为全局变量。\n- 大小写必须严格区分。\n\n#### （5）**让配置文件立即生效**，使用如下命令：\n\n```\n上面修改那个文件就用那个命令\nsource /etc/profile\n\nsource ~/.bashrc \n```\n\n#### （6）检测JDK是否安装成功，使用命令查看JDK版本：\n\n```\njava -version\nwhereis java\n```\n\n执行此命令后，若是出现JDK版本信息说明配置成功\n\n## 2、免密登录\n\n### 配置SSH免密登录\n\n#### （1）下载SSH服务并启动\n\nSSH服务（openssh-server和openssh-clients）已经为大家下载好，所以此处直接启动即可：\n\n```shell\n/usr/sbin/sshd\nsudo apt-get update\nsudo apt install openssh-server\nsudo apt install openssh-clients\nsudo ps -e |grep ssh //查看是否启动如果没有则启动ssh\nsudo service ssh start \n\nsudo vi /etc/ssh/sshd_config\n在行\"#PermitRootLogin prohibit-password\"后\n\n添加行\n\n\"PermitRootLogin yes\"\n\n并保存。\n输入命令\"sudo update-rc.d ssh defaults\"开启ssh服务开机自启动\n输入命令\"sudo service ssh start\"启动服务\n\n输入命令\"sudo service ssh status\"查看服务运行状态\n\n\"active(running)\"说明服务正常\n\n```\n\n安装完毕，运行命令\n\n```\nsudo vi /etc/ssh/sshd_config\n//在行\"#PermitRootLogin prohibit-password\"后添加行并保存。\n\nPermitRootLogin yes\n\n//输入命令开启ssh服务开机自启动\nsudo update-rc.d ssh defaults\n//输入命令启动服务\nsudo service ssh start\n//输入命令查看服务运行状态\nsudo service ssh status\n\n//\"active(running)\"说明服务正常\n```\n\nSSH服务启动成功后，默认开启**22（SSH的默认端口）端口号**，使用以下命令进行从查看：\n\n```shell\nnetstat -tnulp\n```\n\n执行命令，可以看到**22号端口**已经开启，证明我们SSH服务启动成功：\n![image.png](http://assets.qingjiaoclass.com/image/20200206/8PJXrFA1Ij1580965846.png)\n\n只要将SSH服务启动成功，我们就可以**进行远程连接访问**了。\n\n#### （2）**首先生成密钥对，使用命令：**\n\n```shell\nssh-keygen\n## 或者\nssh-keygen -t rsa\n```\n\n上面一种是简写形式，提示要输入信息时不需要输入任何东西，**直接回车三次即可**。\n\n从打印信息中可以看出，私钥id_rsa和公钥id_rsa.pub都已创建成功，并放在 **/root/.ssh**（**隐藏文件夹（以.开头）**）目录中：\n\n#### （3）将公钥放置到**授权列表文件 authorized_keys**中，使用命令：\n\n```shell\ncd /root/.ssh\ncp id_rsa.pub authorized_keys\n```\n\n注意：一定要**授权列表文件 authorized_keys**写对，不能改名。\n\n#### （4）修改授权列表文件 authorized_keys 的权限，使用命令：\n\n```shell\nchmod 600 authorized_keys\n```\n\n设置**拥有者可读可写**，其他人无任何权限（不可读、不可写、不可执行）。\n\n#### （5）验证免密登录是否配置成功，使用如下命令：\n\n```shell\nssh localhost  \n## 或者\nssh e2d670ea9ad7\n## 或者\nssh 10.141.0.42\n```\n\n- **localhost**：意为“本地主机”，指“这台计算机”\n- **e2d670ea9ad7**：本机主机名，可以使用**hostname**命令进行查看\n- **10.141.0.42**：本机IP地址，可以使用**ifconfig**命令进行查看\n\n#### （6）远程登录成功后，若想退出，可以使用**exit**命令。\n\n## 3、安装hadoop\n\n### 1. 进入到/root/software目录，解压Hadoop；\n\n```\ncd /root/software\ntar -zxvf hadoop-2.7.7.tar.gz -C /root/software\n```\n\n### 2. 配置Hadoop系统变量\n\n#### (1) 首先打开/etc/profile文件（系统环境变量：对所有用户有效）：\n\n```shell\nvim /etc/profile或者\n\nvim ~/.bashrc #建议用这种方式\n```\n\n#### (2) 在文件底部添加如下内容：\n\n```shell\n# 配置Hadoop的安装目录\nexport HADOOP_HOME=/root/software/hadoop-2.7.7\n# 在原PATH的基础上加入Hadoop的bin和sbin目录\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n```\n\n生效环境变量：\n\n```\n//上面用那种方式就用那种方式，第二种（亲测）\nsource /etc/profile\n\nsource ~/.bashrc\n```\n\n\n\n```\nwhereis hdfs\nwhereis start-all.sh\n//显示出路径则为环境配置成功\n```\n\n\n\n## 4、配置HDFScor\n\n### 配置HDFS\n\n#### 1. 配置环境变量hadoop-env.sh\n\n打开hadoop-env.sh文件：\n\n```shell\nvim /root/software/hadoop-2.7.7/etc/hadoop/hadoop-env.sh\n```\n\n找到JAVA_HOME参数位置，**修改为本机安装的JDK的实际位置**：\n\n\n\n在命令模式下输入 `:set nu` 可以为vi设置行号。\n\n#### 2. 配置核心组件core-site.xml\n\n该文件是**Hadoop的核心配置文件**，其目的是**配置HDFS地址**、**端口号**，以及**临时文件目录**。使用如下命令**打开“core-site.xml”文件**：\n\n```shell\nvim /root/software/hadoop-2.7.7/etc/hadoop/core-site.xml\n```\n\n将下面的配置内容添加到 `<configuration></configuration>` 中间：\n\n```xml\n<!-- HDFS集群中NameNode的URI（包括协议、主机名称、端口号），默认为 file:/// -->\n<property>\n<name>fs.defaultFS</name>\n<!-- 用于指定NameNode的地址 -->\n<value>hdfs://localhost:9000</value>\n</property>\n<!-- Hadoop运行时产生文件的临时存储目录 -->\n<property>\n<name>hadoop.tmp.dir</name>\n    <!-- 该路径为自己用户 -->\n<value>/root/hadoopData/temp</value>\n</property>\n```\n\n#### 3. 配置文件系统hdfs-site.xml\n\n该文件主要用于**配置 HDFS 相关的属性**，例如**复制因子（即数据块的副本数）**、**NameNode 和 DataNode 用于存储数据的目录**等。在**完全分布式模式**下，默认数据块副本是**3 份**。 使用如下命令**打开“hdfs-site.xml”文件**：\n\n```shell\nvim /root/software/hadoop-2.7.7/etc/hadoop/hdfs-site.xml\n```\n\n将下面的配置内容添加到 `<configuration></configuration>` 中间：\n\n```xml\n<!-- NameNode在本地文件系统中持久存储命名空间和事务日志的路径 -->\n<property>\n<name>dfs.namenode.name.dir</name>\n<value>/root/hadoopData/name</value>\n</property>\n<!-- DataNode在本地文件系统中存放块的路径 -->\n<property>\n<name>dfs.datanode.data.dir</name>\n    <!-- 该路径为自己用户 -->\n<value>/root/hadoopData/data</value>\n</property>\n<!-- 数据块副本的数量，默认为3 -->\n<property>\n<name>dfs.replication</name>\n<value>1</value>\n</property>\n```\n\n#### 4. 配置slaves文件（无需修改）\n\n该文件用于**记录Hadoop集群所有从节点**（HDFS的DataNode*和*YARN的NodeManager所在主机）的主机名，用来配合**一键启动**脚本启动集群从节点（并且还需要保证关联节点配置了**SSH免密登录**）。\n\n打开该配置文件：\n\n```shell\nvim /root/software/hadoop-2.7.7/etc/hadoop/slaves\n```\n\n我们看到其默认内容为**localhost**，因为我们搭建的是**伪分布式集群**，就只有一台主机，所以从节点也需要放在此主机上，所以**此配置文件无需修改**。\n\n#### 5.格式化文件系统\n\n```\nhdfs namenode -format\n```\n\n#### 6. 脚本一键启动hdfs\n\n启动集群**最常使用的方式**是使用脚本一键启动，前提是**需要配置 slaves 配置文件和 SSH免密登录**。\n\n- **在本机上使用如下方式一键启动HDFS集群：**\n\n```shell\nstart-dfs.sh\n```\n\n在本机上执行 `jps` 命令，在打印结果中会看到**4** 个进程，分别是 **NameNode**、**SecondaryNameNode**、**Jps**、和**DataNode**，如果出现了这 4 个进程表示HDFS启动成功。\n\n## 5、安装yarn\n\nYarn主要配置文件说明如下：\n![img](https://assets.qingjiaoclass.com/qyzpbaerkbax/20210414/bkqoojnq_bfZGTPFz1UaOe8TXCf0Q)\n\n### 1. 配置环境变量yarn-env.sh\n\n该文件是**YARN框架运行环境的配置**，同样需要**修改JDK所在位置**。\n\n使用如下命令打开“yarn-env.sh”文件：\n\n```shell\nvim /root/software/hadoop-2.7.7/etc/hadoop/yarn-env.sh\n```\n\n找到JAVA_HOME参数位置，将前面的#去掉，将其值修改为本机安装的JDK的实际位置：\n![image.png](http://assets.qingjiaoclass.com/image/20200207/Zn1Xo7erlh1581045238.png)\n\n在命令模式下输入 `:set nu` 可以为vi设置行号。\n\n### 2. 配置计算框架mapred-site.xml\n\n在$HADOOP_HOME/etc/hadoop/目录中默认没有该文件，需要先通过如下命令将文件**复制并重命名为“mapred-site.xml”**：\n\n```shell\ncp mapred-site.xml.template mapred-site.xml\n```\n\n接着，打开“mapred-site.xml”文件进行修改：\n\n```shell\nvim /root/software/hadoop-2.7.7/etc/hadoop/mapred-site.xml\n```\n\n将下面的配置内容添加到 中间：\n\n```xml\n<!-- 指定使用 YARN 运行 MapReduce 程序，默认为 local -->\n<property>\n<name>mapreduce.framework.name</name>\n<value>yarn</value>\n</property>\n```\n\n效果如下图所示：\n![image.png](http://assets.qingjiaoclass.com/image/20200207/B6gnrtwbWK1581045450.png)\n\n### 3. 配置YARN系统yarn-site.xml\n\n本文件是**YARN框架的核心配置文件**，用于**配置 YARN 进程及 YARN 的相关属性**。\n\n使用如下命令打开该配置文件：\n\n```shell\nvim /root/software/hadoop-2.7.7/etc/hadoop/yarn-site.xml\n```\n\n将下面的配置内容加入中间：\n\n```xml\n<!-- NodeManager上运行的附属服务，也可以理解为 mapreduce 获取数据的方式 -->\n<property>\n<name>yarn.nodemanager.aux-services</name>\n<value>mapreduce_shuffle</value>\n</property>\n```\n\n### 4. 启动集群\n\n在本机上使用如下方式**一键启动YARN集群**：\n\n```shell\nstart-yarn.sh\n```\n\nps：start-dfs.sh和start-yarn.sh也是sbin目录下的脚本文件。\n\n**效果图如下所示：**\n\n**打印信息：**\n\n- 在本机上启动了 ResourceManager守护进程\n- 在本机上启动了 NodeManager 守护进程\n\n通过本机的浏览器访问**http://localhost:8088**或**http://本机IP地址:8088**查看YARN集群状态，效果如下图所示：\n\n## 测试hadoop\n\n```\n创建一个data.txt用wordcount测试\nhadoop@hadoop:~$ vi data.txt\n创建input文件加夹\nhadoop@hadoop:~$ hdfs dfs -mkdir /input\n\n\n```\n\nhdfs命令\n\n```\nhdfs dfs -linux命令 /input或者/output\n\n```\n\n上传data.txt\n\n```\nhadoop@hadoop:~$ hdfs dfs -put data.txt /input\n\n\n```\n\n查看上传是否成功\n\n```\nhadoop@hadoop:~$ hdfs dfs -ls /input\nFound 1 items\n-rw-r--r--   1 hadoop supergroup         25 2021-04-30 23:47 /input/data.txt\n\n\n```\n\n运行wordCount程序\n\n```\nhadoop@hadoop:~$ cd ~/software/hadoop-2.7.7/share/hadoop/mapreduce\nhadoop@hadoop:~/software/hadoop-2.7.7/share/hadoop/mapreduce$ hadoop jar hadoop-mapreduce-examples-2.7.7.jar wordcount /input/data.txt /output\n\n```\n\n查看output文件夹\n\n```\nhdfs dfs -ls /output\nFound 2 items\n-rw-r--r--   1 hadoop supergroup          0 2021-04-30 23:56 /output/_SUCCESS\n-rw-r--r--   1 hadoop supergroup         25 \n\n\n```\n\n查看运行结果\n\n```\n2021-04-30 23:56 /output/part-r-00000\nhadoop@hadoop:~/software/hadoop-2.7.7/share/hadoop/mapreduce$ hdfs dfs -cat /output/part-r-00000\nhadoop\t1\nhello\t2\nworld\t1\n\n```\n\n","tags":["-hadoop"],"categories":["hadoop"]},{"title":"Hbase安装","url":"/2021/05/06/Hbase安装/","content":"\n# Hbase实验一\n\nzookeeper-3.4.14\n\n**~/.bashrc 内容**\n\n```\nsoftware下的解压后创建软连接避免输入错误\n例：\nin -s hbase-2.2.7 hbase\n...jdk\n...hadoop\n```\n\n<!--more-->\n\n```\n# 配置Hadoop的安装目录\nexport HADOOP_HOME=/home/hadoop/software/hadoop\n# 在原PATH的基础上加入Hadoop的bin和sbin目录\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\nexport JAVA_HOME=/home/hadoop/software/jdk  # 配置Java的安装目录\nexport PATH=$PATH:$JAVA_HOME/bin  # 在原PATH的基础上加入JDK的bin目录\nexport HBASE_HOME=/home/hadoop/software/hbase\nexport PATH=$PATH:$HBASE_HOME/bin\nexport ZOOKEEPER_HOME=/home/hadoop/software/zookeeper\nexport PATH=$PATH:$ZOOKEEPER_HOME/bin\n\n```\n\n\n\n### 1、HBase的安装\n\n##### 解压hbase\n\n 进入/home/hadoop/**software**\n\n```\n tar -zxvf hbase-2.7.7-bin.tar.gz\n```\n\n##### 配置环境变量\n\n```\nvi ~/.bashrc\n```\n\n```\nexport HBASE_HOME=/home/hadoop/software/hbase\nexport PATH=$PATH:$HBASE_HOME/bin\n\n```\n\n##### 使环境变量生效\n\n```\nsource ~/.bashrc\n```\n\n### 2、伪分布式集群\n\n##### 修改hbase-env.sh\n\n```\ncd ~/hbase/conf\nvi hbase-env.sh\n```\n\n打开在其中添加\n\n```\nexport JAVA_HOME=/home/hadoop/software/jdk\nexport HBASE_MANAGES_ZK=true\n```\n\n##### 修改hbase-env.sh\n\n```\ncd ~/hbase/conf\nvi hbase-site.xml\n```\n\n修改为 \n\n**注：hdfs节点名称与hdfs的core-site.xml文件相同\n\n```\n<property>\n<name>hbase.rootdir</name>\n<value>hdfs://localhost:9000/hbase</value>\n</property>\n\n<property>\n<name>hbase.cluster.distributed</name>\n<value>true</value>\n</property>\n\n<property>\n<name>hbase.zookeeper.quorum</name>\n<value>hadoop</value>\n</property>\n\n<property>\n<name>dfs.replication</name>\n<value>1</value>\n</property>\n\n<property>\n<name>hbase.zookeeper.property.dataDir</name>\n<value>/home/hadoop/software/hbase/zookeeper</value>\n</property>\n\n\n\n\n```\n\n##### 启动HBase\n\n```\nstart-hbase.sh\n```\n\n##### 检查进程\n\n```\njps\n```\n\n"},{"title":"Hello World","url":"/2021/05/06/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n<!--more-->\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n"}]